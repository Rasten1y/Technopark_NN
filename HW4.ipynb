{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJu1PdoRlu6B"
   },
   "source": [
    "## Семинар: \"Методы оптимизации\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJvEPbiSlu6F"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from torchvision import datasets, transforms\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "plt.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YG-sht8Olu6G"
   },
   "source": [
    "На этом семинаре мы попробуем реализовать и сравнить различные методы оптимизации: SGD, Momentum, NAG, Adagrad, RMSProp, AdaDelta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B8vy8i7lu6G"
   },
   "source": [
    "### Реализация методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7ukIsNclu6G"
   },
   "source": [
    "Для всех экспериментов подберите параметры так, чтобы метод сошелся к ближайшему локальному минимуму. Все методы следует запускать из одной и той же точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtBxHAqTlu6G"
   },
   "outputs": [],
   "source": [
    "# https://github.com/dzlab/deepprojects/blob/master/visualization/Optimizers_in_Action.ipynb\n",
    "\n",
    "def grid_samples(center=[0, 0], offset=5, size=100):\n",
    "    range1 = np.linspace(center[0]-offset, center[0]+offset, size)\n",
    "    range2 = np.linspace(center[1]-offset, center[1]+offset, size)\n",
    "    return torch.from_numpy(np.stack(np.meshgrid(range1, range2))).float()\n",
    "\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return ((y - y_hat) ** 2).mean(axis=-1)\n",
    "\n",
    "\n",
    "def msre(y, y_hat):\n",
    "    return ((y - y_hat) ** 2).mean(axis=-1).sqrt()\n",
    "\n",
    "\n",
    "def mae(y, y_hat):\n",
    "    return ((y - y_hat).abs()).mean(axis=-1)\n",
    "\n",
    "\n",
    "class LossAnimator:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.size = len(X)\n",
    "\n",
    "    def loss_func(self, W, loss):\n",
    "        shape = W.shape\n",
    "        return loss((self.X @ W.view(shape[0], -1)).T, self.y).view(shape[1:])\n",
    "\n",
    "    def plot_loss_funcs(self, weights, fcts, titles, view=(20, 50)):\n",
    "        num_fcts = len(fcts)\n",
    "        fig = plt.figure(figsize=(7 * num_fcts,7))\n",
    "        for i in range(num_fcts):\n",
    "            loss = self.loss_func(weights, loss=fcts[i])\n",
    "            ax = fig.add_subplot(1, num_fcts, i+1, projection='3d')\n",
    "            ax.plot_surface(*weights, loss, cmap='viridis')\n",
    "            ax.set_xlabel('w0'); ax.set_ylabel('w1'); ax.set_zlabel('Loss')\n",
    "            ax.set_title(titles[i])\n",
    "            ax.view_init(*view)\n",
    "\n",
    "    def _init_animation(self, epochs, train_data):\n",
    "        self.train_data = train_data\n",
    "        self.epochs = epochs\n",
    "        self.nmethods = len(train_data)\n",
    "\n",
    "        weights = grid_samples(offset=5)\n",
    "\n",
    "        max_loss = max([data['losses'].max() for data in train_data.values()])\n",
    "        loss_curve = self.loss_func(weights, loss=mse)\n",
    "        colors = cm.rainbow(np.linspace(0, 1, self.nmethods))\n",
    "\n",
    "\n",
    "        self.fig = plt.figure(figsize=(14, 8))\n",
    "        self.gs = GridSpec(2, 2, width_ratios=[1, 2.5])\n",
    "\n",
    "        self.ax0 = self.fig.add_subplot(self.gs[0,0])\n",
    "        self.lines0 = {\n",
    "            name: self.ax0.plot([], [], c=c, label=name)[0]\n",
    "            for name, c in zip(train_data.keys(), colors)\n",
    "        }\n",
    "        self.ax0.scatter(self.X[:,0], self.y, c='orange', label='Ground truth')\n",
    "        self.ax0.set_ylim(self.y.min(), self.y.max())\n",
    "        self.ax0.set_title('Ground truth & Model', fontsize=16)\n",
    "        self.ax0.legend(loc='lower right')\n",
    "\n",
    "        self.ax1 = self.fig.add_subplot(self.gs[:,1], projection='3d')\n",
    "        self.ax1.plot_surface(*weights, loss_curve-0.5, cmap='viridis', alpha=0.8)\n",
    "        self.ax1.view_init(50, 70)\n",
    "        self.lines1 = {\n",
    "            name: self.ax1.plot3D([], [], [], c=c, marker='o', alpha=0.9, label=name)[0]\n",
    "            for name, c in zip(train_data.keys(), colors)\n",
    "        }\n",
    "        self.ax1.set_title('Loss', fontsize=16, pad=20)\n",
    "        self.ax1.set_xlabel('w0')\n",
    "        self.ax1.set_ylabel('w1')\n",
    "        self.ax1.set_zlabel('Loss')\n",
    "        self.ax1.legend()\n",
    "\n",
    "        self.ax2 = self.fig.add_subplot(self.gs[1,0])\n",
    "        self.lines2 = {\n",
    "            name: self.ax2.plot([], [], c=c, label=name)[0]\n",
    "            for name, c in zip(train_data.keys(), colors)\n",
    "        }\n",
    "        self.ax2.set_title('Loss', fontsize=16)\n",
    "        self.ax2.set_ylabel('loss')\n",
    "        self.ax2.set_ybound(0, max_loss)\n",
    "        self.ax2.set_xlim(0, epochs)\n",
    "        self.ax2.legend(loc='center right')\n",
    "\n",
    "        self.fig.tight_layout()\n",
    "        self.fig.subplots_adjust(top=0.85)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        return self.fig\n",
    "\n",
    "    def _animate(self, i):\n",
    "        steps = np.arange(i+1)\n",
    "        left = max(0, i-20)\n",
    "        for name, data in self.train_data.items():\n",
    "            # plot ground truth & model\n",
    "            self.lines0[name].set_data(self.X[:, 0], self.X @ data['weights'][i])\n",
    "\n",
    "            # plot loss (output of the sampling)\n",
    "            self.lines1[name].set_data(data['weights'][left:i+1, 0], data['weights'][left:i+1, 1])\n",
    "            self.lines1[name].set_3d_properties(data['losses'][left:i+1])\n",
    "\n",
    "            self.lines2[name].set_data(steps, data['losses'][:i+1])\n",
    "\n",
    "        self.fig.suptitle(f'Epoch: {i}/{self.epochs}', fontsize=22)\n",
    "\n",
    "    def animate(self, epochs, train_data, step_skip=1):\n",
    "        self._init_animation(epochs, train_data)\n",
    "        anim = animation.FuncAnimation(\n",
    "            self.fig, self._animate,\n",
    "            frames=range(0, epochs, step_skip),\n",
    "            interval=100 * step_skip\n",
    "        )\n",
    "        return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "_Q-AKGojlu6H",
    "outputId": "5c66586a-65c8-421d-b416-2e4e5582c523"
   },
   "outputs": [],
   "source": [
    "size = 100\n",
    "X = torch.ones(size, 2)\n",
    "X[:, 0].uniform_(-1., 1)\n",
    "\n",
    "y_hat = 3 * X[:, 0] + 2\n",
    "y = y_hat + torch.randn(size)\n",
    "\n",
    "loss_animator = LossAnimator(X, y)\n",
    "\n",
    "plt.scatter(loss_animator.X[:,0], loss_animator.y, label='y');\n",
    "plt.scatter(loss_animator.X[:,0], y_hat, label='y_hat');\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "lxosC3Kmlu6H",
    "outputId": "4f7cb9be-4500-430c-d386-8b674b81c344"
   },
   "outputs": [],
   "source": [
    "weights = grid_samples()\n",
    "loss_animator.plot_loss_funcs(weights, [mse, msre, mae], ['MSE', 'MSRE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9EZCcVIlu6I"
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = list(params)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.detach_()\n",
    "                param.grad.zero_()\n",
    "\n",
    "    def pre_step(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "    def update_param(self, p):\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbDX6AVelu6I"
   },
   "outputs": [],
   "source": [
    "def optimize_function(fn, optim, optim_args, start_point, num_iter = 50):\n",
    "    weigths = nn.Parameter(torch.FloatTensor(start_point), requires_grad=True)\n",
    "\n",
    "    optim = optim(params=[weigths], **optim_args)\n",
    "    points = []\n",
    "    losses = []\n",
    "    for i in range(num_iter):\n",
    "        if hasattr(optim, 'pre_step'):\n",
    "            optim.pre_step()\n",
    "        loss = fn(weigths)\n",
    "        points.append(weigths.data.detach().clone())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    points = torch.stack(points, axis=0)\n",
    "    losses = torch.FloatTensor(losses)\n",
    "    return points, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4mT9Rqhlu6I"
   },
   "outputs": [],
   "source": [
    "def compare_optimizers(\n",
    "    loss_animator,\n",
    "    fn,\n",
    "    optim_list,\n",
    "    start_point,\n",
    "    num_iter = 50,\n",
    "    step_skip = 1\n",
    "):\n",
    "    data = {}\n",
    "    loss_func = partial(loss_animator.loss_func, loss=fn)\n",
    "    for name, optim, args in tqdm(optim_list):\n",
    "        points, losses = optimize_function(loss_func, optim, args, start_point, num_iter)\n",
    "        data[name] = {\n",
    "            'weights': points,\n",
    "            'losses': losses,\n",
    "        }\n",
    "\n",
    "    return loss_animator.animate(num_iter, data, step_skip=step_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bkwGE-ffEKh"
   },
   "source": [
    "#### SGD\n",
    "$$\\theta_t = \\theta_{t-1} - \\eta \\sum_{i_1, ..., i_k} \\nabla_{\\theta} J_{i} (\\theta_{t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkMEEoCdlu6I"
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=1e-2):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p -= self.lr * p.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsnWEyPMf956"
   },
   "source": [
    "#### Momentum\n",
    "$$\\nu_t=\\gamma \\nu_{t-1} + \\eta_t \\nabla_{\\theta} J_{i} (\\theta_{t-1})$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\nu_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjVUBzJUyi3B"
   },
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self, params, lr=1e-2, gamma=0.9):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.prev_momentum = [torch.zeros(p.shape) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                prev_momentum = self.prev_momentum[i]\n",
    "                self.prev_momentum[i] = self.gamma * prev_momentum + self.lr * p.grad\n",
    "                p -= self.prev_momentum[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzFzGKrEgmPO"
   },
   "source": [
    "#### Nesterov Accelerated Gradient (NAG)\n",
    "$$\\nu_t=\\gamma \\nu_{t-1} + \\eta_t \\nabla_{\\theta} J_{i} (\\theta_{t-1} - \\gamma \\nu_{t-1})$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\nu_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeFSz06Wlu6J"
   },
   "outputs": [],
   "source": [
    "class NAG(Optimizer):\n",
    "    def __init__(self, params, lr=1e-2, gamma=0.9):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.prev_momentum = [torch.zeros(p.shape) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                prev_momentum = self.prev_momentum[i]\n",
    "                self.prev_momentum[i] = self.gamma * prev_momentum + self.lr * p.grad\n",
    "                p -= (1 + self.gamma) * self.prev_momentum[i] - self.gamma * prev_momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do94tP_ag4RM"
   },
   "source": [
    "#### AdaGrad\n",
    "$$G_t = \\sum_{k=0}^t g_{k}^2$$\n",
    "$$\\theta_{t} = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{G_{t-1} + ϵ}} \\cdot g_{t-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh8KIMyBe8xB"
   },
   "outputs": [],
   "source": [
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, params, epsilon=1e-8, eta=1e-2):\n",
    "        super().__init__(params)\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "\n",
    "        self.G = [torch.zeros(p.shape) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.G[i] += p.grad ** 2\n",
    "                p -= self.eta * p.grad / torch.sqrt(self.G[i] + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWHmOwuJhWss"
   },
   "source": [
    "#### RMSProp\n",
    "$$G_t = \\gamma G_{t-1} + (1 - \\gamma) g_t^2$$\n",
    "$$\\theta_{t} = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{G_{t-1} + ϵ}} \\cdot g_{t-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hen52tFDe-lc"
   },
   "outputs": [],
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params, epsilon=1e-8, eta=1e-2, gamma=0.9):\n",
    "        super().__init__(params)\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.G = [torch.zeros(p.shape) for p in self.params]\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                self.G[i] = self.gamma * self.G[i] + (1 - self.gamma) * (p.grad ** 2)\n",
    "                p -= self.eta / torch.sqrt(self.G[i] + self.epsilon) * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iodrNjOwfDWh"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "points, losses = optimize_function(\n",
    "    partial(loss_animator.loss_func, loss=mse),\n",
    "    SGD,\n",
    "     {'lr': 1e-2},\n",
    "    start_point=[-4, -4],\n",
    "    num_iter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhpHrdwpfFzO"
   },
   "outputs": [],
   "source": [
    "compare_optimizers(\n",
    "    loss_animator,\n",
    "    mae,\n",
    "    [\n",
    "        ['SGD', SGD, {'lr': 1e-1}],\n",
    "        ['Momentum', NAG, {'lr': 1e-1}],\n",
    "        ['NAG', NAG, {'lr': 1e-1}],\n",
    "        ['AdaGrad', AdaGrad, {'eta' : 1}],\n",
    "        ['RMSProp', RMSProp, {'eta' : 1}],\n",
    "    ],\n",
    "    start_point=[-4, -4],\n",
    "    num_iter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF5_fAP2lu6J"
   },
   "source": [
    "###  Домашнее задание: Обучение нейронной сети + Подбор шага"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqJRnJWGlu6J"
   },
   "source": [
    "В этом задании вам нужно:\n",
    "1) Реализовать Adam, AdamW и обучить сверточную нейросеть на MNIST. Сравнить графики обучения для SGD, Adam и AdamW. **(2 балла)**\n",
    "2) Реализовать LinearLR и CosineAnnealingLR. Обучить сверточную нейросеть на MNIST с SGD, Adam, AdamW с LinearLR, CosineAnnealingLR. Сравните графики изменения loss-функции и точности (суммарно должно быть 9 экспериментов, 3 из которых покрываются пунктом 1). **(3 балла)**\n",
    "\n",
    "Ссылки:\n",
    "- https://arxiv.org/abs/1711.05101 (AdamW, статья)\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html (AdamW, PyTorch)\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html (LinearLR, PyTorch)\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html (CosineAnnealingLR, PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOSDfkamlu6J",
    "outputId": "072b96c4-5fa8-49c8-b5ca-0d0b92a3492e"
   },
   "outputs": [],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdAZhwGHlu6J"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('.', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVpfWvvNjt9v"
   },
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr = 1e-2, beta1 = 0.9, beta2 = 0.999, eta = 1e-8):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eta = eta\n",
    "        self.prev_momentum = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.G = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                prev_momentum = self.prev_momentum[i]\n",
    "                self.prev_momentum[i] = self.beta1 * prev_momentum + (1 - self.beta1) * p.grad\n",
    "                self.G[i] = self.beta2 * self.G[i] + (1 - self.beta2) * (p.grad ** 2)\n",
    "                m_n = self.prev_momentum[i] / (1 - self.beta1 ** self.t)\n",
    "                v_n = self.G[i] / (1 - self.beta2 ** self.t)\n",
    "                p -= self.lr * m_n / (v_n ** 0.5 + self.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr = 1e-2, wd = 1e-1, beta1 = 0.9, beta2 = 0.999, eta = 1e-8):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eta = eta\n",
    "        self.prev_momentum = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.G = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                prev_momentum = self.prev_momentum[i]\n",
    "                self.prev_momentum[i] = self.beta1 * prev_momentum + (1 - self.beta1) * p.grad\n",
    "                self.G[i] = self.beta2 * self.G[i] + (1 - self.beta2) * (p.grad ** 2)\n",
    "                m_n = self.prev_momentum[i] / (1 - self.beta1 ** self.t)\n",
    "                v_n = self.G[i] / (1 - self.beta2 ** self.t)\n",
    "                p -= self.lr * m_n / (v_n ** 0.5 + self.eta)\n",
    "                p -= self.lr * self.wd * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_optimizers(\n",
    "    loss_animator,\n",
    "    mae,\n",
    "    [\n",
    "        ['SGD', SGD, {'lr': 1e-1}],\n",
    "        ['Adam', Adam, {'lr': 1e-1}],\n",
    "        ['AdamW', AdamW, {'lr': 1e-1}],\n",
    "    ],\n",
    "    start_point=[-4, -4],\n",
    "    num_iter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EGxA_GGlu6J"
   },
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, mode=\"const\", total_iters=100, start_factor=0.1, end_factor=1.0, eta_min=1e-5):\n",
    "        pass #Пытался через Scheduler, но не смог до конца довести\n",
    "\n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AuwsFOCim3P"
   },
   "outputs": [],
   "source": [
    "class LinearLR(LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: Optimizer,\n",
    "        start_factor: float = 0.1,\n",
    "        end_factor: float = 1.0,\n",
    "        total_iters: int = 100\n",
    "    ):\n",
    "        super().__init__(optimizer)\n",
    "        self.start_factor = start_factor\n",
    "        self.end_factor = end_factor\n",
    "        self.total_iters = total_iters\n",
    "        \n",
    "    def get_factor(self, t: int) -> float:\n",
    "        if self.total_iters <= 0:\n",
    "            return self.end_factor\n",
    "        t_eff = min(max(t, 0), self.total_iters)\n",
    "        return self.start_factor + (t_eff / self.total_iters) * (self.end_factor - self.start_factor)\n",
    "        \n",
    "\n",
    "class CosineAnnealingLR(LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: Optimizer,\n",
    "        T_max: int = 100,\n",
    "        eta_min: float = 0.0\n",
    "    ):\n",
    "        super().__init__(optimizer)\n",
    "        self.T_max = int(T_max)\n",
    "        self.eta_min = float(eta_min)\n",
    "\n",
    "    def get_factor(self, t: int) -> float:\n",
    "        if self.T_max <= 0:\n",
    "            return self.eta_min / self.base_lr\n",
    "        t_eff = min(max(t, 0), self.T_max)\n",
    "        return (self.eta_min / self.base_lr) + 0.5 * (1 - self.eta_min / self.base_lr) * (1 + math.cos(math.pi * t_eff / self.T_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDLinear(Optimizer):\n",
    "    def __init__(self, params, base = 5e-3, end = 1e-2, n_steps = 10):\n",
    "        super().__init__(params)\n",
    "        self.t = 0\n",
    "        self.base = base\n",
    "        self.lr = base\n",
    "        self.end = end\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.n_steps:\n",
    "            self.lr += (self.end - self.base) / self.n_steps\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamLinear(Optimizer):\n",
    "    def __init__(self, params, base = 5e-3, end = 1e-2, n_steps = 10, beta1 = 0.9, beta2 = 0.999, eta = 1e-8):\n",
    "        super().__init__(params)\n",
    "        self.base = base\n",
    "        self.lr = base\n",
    "        self.end = end\n",
    "        self.n_steps = n_steps\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eta = eta\n",
    "        self.prev_momentum = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.G = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.n_steps:\n",
    "            self.lr += (self.end - self.base) / self.n_steps\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                prev_momentum = self.prev_momentum[i]\n",
    "                self.prev_momentum[i] = self.beta1 * prev_momentum + (1 - self.beta1) * p.grad\n",
    "                self.G[i] = self.beta2 * self.G[i] + (1 - self.beta2) * (p.grad ** 2)\n",
    "                m_n = self.prev_momentum[i] / (1 - self.beta1 ** self.t)\n",
    "                v_n = self.G[i] / (1 - self.beta2 ** self.t)\n",
    "                p -= self.lr * m_n / (v_n ** 0.5 + self.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWLinear(Optimizer):\n",
    "    def __init__(self, params, base = 5e-3, end = 1e-2, n_steps = 10, wd = 1e-1, beta1 = 0.9, beta2 = 0.999, eta = 1e-8):\n",
    "        super().__init__(params)\n",
    "        self.base = base\n",
    "        self.lr = base\n",
    "        self.end = end\n",
    "        self.n_steps = n_steps\n",
    "        self.wd = wd\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eta = eta\n",
    "        self.prev_momentum = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.G = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.n_steps:\n",
    "            self.lr += (self.end - self.base) / self.n_steps\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                prev_momentum = self.prev_momentum[i]\n",
    "                self.prev_momentum[i] = self.beta1 * prev_momentum + (1 - self.beta1) * p.grad\n",
    "                self.G[i] = self.beta2 * self.G[i] + (1 - self.beta2) * (p.grad ** 2)\n",
    "                m_n = self.prev_momentum[i] / (1 - self.beta1 ** self.t)\n",
    "                v_n = self.G[i] / (1 - self.beta2 ** self.t)\n",
    "                p -= self.lr * m_n / (v_n ** 0.5 + self.eta)\n",
    "                p -= self.lr * self.wd * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_optimizers(\n",
    "    loss_animator,\n",
    "    mae,\n",
    "    [\n",
    "        ['SGDLinear', SGDLinear, {'base': 5e-2,  'end': 5e-1}],\n",
    "        ['AdamLinear', AdamLinear, {'base': 5e-2,  'end': 5e-1}],\n",
    "        ['AdamWLinear', AdamWLinear, {'base': 5e-2,  'end': 5e-1}],\n",
    "    ],\n",
    "    start_point=[-4, -4],\n",
    "    num_iter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDCosine(Optimizer):\n",
    "    def __init__(self, params, base = 5e-3, end = 1e-2, n_steps = 10):\n",
    "        super().__init__(params)\n",
    "        self.t = 0\n",
    "        self.base = base\n",
    "        self.lr = base\n",
    "        self.end = end\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.n_steps:\n",
    "            self.lr = self.base + (self.end - self.base) * (1 + np.cos(self.t * np.pi / self.n_steps)) / 2\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamCosine(Optimizer):\n",
    "    def __init__(self, params, base = 5e-3, end = 1e-2, n_steps = 10, beta1 = 0.9, beta2 = 0.999, eta = 1e-8):\n",
    "        super().__init__(params)\n",
    "        self.base = base\n",
    "        self.lr = base\n",
    "        self.end = end\n",
    "        self.n_steps = n_steps\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eta = eta\n",
    "        self.prev_momentum = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.G = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.n_steps:\n",
    "            self.lr = self.base + (self.end - self.base) * (1 + np.cos(self.t * np.pi / self.n_steps)) / 2\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                prev_momentum = self.prev_momentum[i]\n",
    "                self.prev_momentum[i] = self.beta1 * prev_momentum + (1 - self.beta1) * p.grad\n",
    "                self.G[i] = self.beta2 * self.G[i] + (1 - self.beta2) * (p.grad ** 2)\n",
    "                m_n = self.prev_momentum[i] / (1 - self.beta1 ** self.t)\n",
    "                v_n = self.G[i] / (1 - self.beta2 ** self.t)\n",
    "                p -= self.lr * m_n / (v_n ** 0.5 + self.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWCosine(Optimizer):\n",
    "    def __init__(self, params, base = 5e-3, end = 1e-2, n_steps = 10, wd = 1e-1, beta1 = 0.9, beta2 = 0.999, eta = 1e-8):\n",
    "        super().__init__(params)\n",
    "        self.base = base\n",
    "        self.lr = base\n",
    "        self.end = end\n",
    "        self.n_steps = n_steps\n",
    "        self.wd = wd\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eta = eta\n",
    "        self.prev_momentum = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.G = [torch.zeros(p.shape) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.n_steps:\n",
    "            self.lr = self.base + (self.end - self.base) * (1 + np.cos(self.t * np.pi / self.n_steps)) / 2\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                prev_momentum = self.prev_momentum[i]\n",
    "                self.prev_momentum[i] = self.beta1 * prev_momentum + (1 - self.beta1) * p.grad\n",
    "                self.G[i] = self.beta2 * self.G[i] + (1 - self.beta2) * (p.grad ** 2)\n",
    "                m_n = self.prev_momentum[i] / (1 - self.beta1 ** self.t)\n",
    "                v_n = self.G[i] / (1 - self.beta2 ** self.t)\n",
    "                p -= self.lr * m_n / (v_n ** 0.5 + self.eta)\n",
    "                p -= self.lr * self.wd * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_optimizers(\n",
    "    loss_animator,\n",
    "    mae,\n",
    "    [\n",
    "        ['SGDCosine', SGDCosine, {'base': 5e-2,  'end': 5e-1}],\n",
    "        ['AdamCosine', AdamCosine, {'base': 5e-2,  'end': 5e-1}],\n",
    "        ['AdamWCosine', AdamWCosine, {'base': 5e-2,  'end': 5e-1}],\n",
    "    ],\n",
    "    start_point=[-4, -4],\n",
    "    num_iter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnd0Yoc_lu6J"
   },
   "source": [
    "#### Feedback (опционально)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O9LOwZOlu6J"
   },
   "source": [
    "Здесь вы можете оставить список опечаток из лекции или семинара:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWde3DuEemdN"
   },
   "source": [
    "- Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfvzYp2Nlu6J"
   },
   "source": [
    "Здесь вы можете оставить комментарии по лекции или семинару:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr6NMQUZetch"
   },
   "source": [
    "- Your text here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "vscode": {
   "interpreter": {
    "hash": "87f2c61721633f38491d53c2bfdcc0361799add3518de6ab4d9ab97e249bd1f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
